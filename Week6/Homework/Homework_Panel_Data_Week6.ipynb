{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework on Panel Data\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "**Use exercise notebook `An_introduction_to_Panel_Data.ipynb' in the Week 6 folder as a guide to complete this notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install panelsplit > /dev/null # installation of PanelSplit\n",
    "# !pip install wbdata > /dev/null # installation of wbdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panelsplit import PanelSplit\n",
    "import wbdata\n",
    "import pandas as pd; import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the example dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd; import numpy as np\n",
    "\n",
    "# Define indicators and countries of interest\n",
    "indicators = {\"NY.GDP.MKTP.CD\": \"GDP\",  # Gross Domestic Product (current US$)\n",
    "              \"SP.POP.TOTL\": \"Population\",  # Total population\n",
    "              \"FP.CPI.TOTL.ZG\": \"Inflation\",     # Inflation, consumer prices (annual %)\n",
    "              \"NE.TRD.GNFS.ZS\": \"Trade_Balance\"}    # Trade balance (% of GDP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select as countries following list: USA, Brazil, Russian Federation, India, China using the correct abbreviations.\n",
    "\n",
    "countries = [\n",
    "    \n",
    "             # Fill out with your code! \n",
    "             \n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target  = ['Population','Inflation','Trade_Balance'], 'GDP' # Define feature columns and target\n",
    "\n",
    "# Fetch data from the World Bank API\n",
    "df = wbdata.get_dataframe(indicators, country=countries).reset_index()\n",
    "df['date'] = df.date.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose as date range the years between 2003 (excludsing 2003) and 2023 (including 2023)\n",
    "\n",
    "df = df.query(\n",
    "              \n",
    "                # Fill out with your code! \n",
    "              \n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('a sample of the data:')\n",
    "display(df.sample(5))\n",
    "\n",
    "print('summary statistics:')\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we don't have all the data available- 10% of the features are missing at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select three different ratios of missing values for the three input features ['Population', 'Inflation', 'Trade_Balance']:\n",
    "# Select the ratios 0.1, 0.12, 0.15, respectively.\n",
    "\n",
    "df.loc[df.sample(\n",
    "    \n",
    "                # Fill out with your code! \n",
    "                 \n",
    "                 ).index, features[0]] = np.nan\n",
    "\n",
    "df.loc[df.sample(\n",
    "    \n",
    "                # Fill out with your code! \n",
    "                 \n",
    "                 ).index, features[1]] = np.nan\n",
    "\n",
    "df.loc[df.sample(\n",
    "    \n",
    "                # Fill out with your code! \n",
    "                 \n",
    "                 ).index, features[2]] = np.nan\n",
    "\n",
    "print('proportion of features that are missing:')\n",
    "round(df[features].isna().mean(), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use PanelSplit to apply imputation in a time-series fashion to fill in the missing data.\n",
    "\n",
    "#### Initializing PanelSplit\n",
    "- To initialize PanelSplit, we pass the time series to the periods argument.\n",
    "- n_splits, gap, and test_size are all arguments used by [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) to split up the time series.\n",
    "- When we specify `plot=True`, a graph is displayed. This is a helpful visualization to understand what this particular instance of PanelSplit is doing. By reading this graph, we see that there are 19 folds or splits, that range from 2003 up to 2023. Train sets are marked in blue and test sets are marked in red.\n",
    "\n",
    "#### `cross_val_fit_transform`\n",
    "- PanelSplit's cross_val_fit_transform takes any object that can implement fit/transform (like a scaler or imputer). \n",
    "- It iteratively fits on the train and transforms the test for each fold. \n",
    "- By setting `include_test_in_fit = True`, we also include the test data during fitting for each fold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the appropriate split for the above date range of 2003 - 2023:\n",
    "\n",
    "n_splits =          # Fill out with your code! \n",
    "\n",
    "panel_split = PanelSplit(periods=df.date, n_splits = n_splits, gap=0, test_size=1, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a different imputation technique: Use SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "df[features], fitted_imputers = panel_split.cross_val_fit_transform(\n",
    "    \n",
    "    # Fill out with your code! \n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only observations which are still missing are in 2004. This is because that is the train set for first split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go ahead and implement imputation for observations that take place in January of 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the imputation for year 2004 and make sure that there are no remaining missing values:\n",
    "\n",
    "df.loc[df.date == 2004, features] =         # Fill out with your code! \n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all observations have all features, we can determine the best set of hyper-parameters to use.\n",
    "\n",
    "#### Hyper-parameter search\n",
    "\n",
    "Since PanelSplit is compatible with sklearn, we can use any sklearn hyper-parameter optimizer has takes cv as an argument.\n",
    "\n",
    "Let's say that our validation period is 2013-2022. Exclude the data for year 2023 from the validationn data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As model choose the XGBoost regressor.\n",
    "\n",
    "# Choose a reasonable parameter grid for the 3 hyperparameters 'max_depth', 'min_child_weight', 'max_leaves'.\n",
    "\n",
    "# Select all data up to excluding the year 2023 as validation data.\n",
    "\n",
    "# Select 10 splits. \n",
    "\n",
    "# Perform your hyperparameter tuning via grid search to get the best set of hyperparameters.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "validation_df = df.query('date < 2023')\n",
    "\n",
    "param_grid =            # Fill out with your code! \n",
    "\n",
    "panel_split = PanelSplit(validation_df.date, n_splits=10, gap=0, test_size=1, plot=True)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    \n",
    "    # Fill out with your code! \n",
    "    \n",
    "    )\n",
    "\n",
    "grid_search.fit(validation_df[features], validation_df[target])\n",
    "\n",
    "print('GridSearch results:')\n",
    "display(pd.DataFrame(grid_search.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the best fit parameters:\n",
    "\n",
    "best_params =           # Fill out with your code! \n",
    "\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've determined what hyper-parameters to use, let's generate predictions in the test period.\n",
    "\n",
    "#### `cross_val_fit_predict`\n",
    "- PanelSplit's cross_val_fit_predict iteratively fits on the train and predicts the test for each fold\n",
    "- Here are the arguments I use:\n",
    "    - estimator: estimator object.\n",
    "    - X: Features.\n",
    "    - y: Target variable.\n",
    "    - labels: pandas DataFrame containing labels for the target variable predicted by the model. The predicted target will be a new column added to this DataFrame.\n",
    "    - prediction_method: The prediction method to use. It can be 'predict', 'predict_proba', or 'predict_log_proba'. Default is 'predict'.\n",
    "    - y_pred_col: Column name for the predicted values. Default is None, in which case the name will be the name of y.name + 'pred'. If y does not have a name attribute, the name will be 'y_pred'.\n",
    "    - n_jobs: The number of jobs to run in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose 10 splits.\n",
    "\n",
    "# Perform XGBoost regression prediction via cross validation.\n",
    "\n",
    "# Note: For `panel_split.cross_val_fit_predict(...)` use the complete dataframe `df`.\n",
    "\n",
    "panel_split = PanelSplit(\n",
    "    \n",
    "    # Fill out with your code! \n",
    "    \n",
    "    )\n",
    "\n",
    "result_df = panel_split.gen_test_labels(df[['country','date', 'GDP']])\n",
    "\n",
    "result_df['GDP_pred'], models = panel_split.cross_val_fit_predict(\n",
    "   \n",
    "                                                                   # Fill out with your code! \n",
    "                                                                      \n",
    "                                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of cross_val_fit_predict is a dataframe of predictions and fitted models. This is what the DataFrame looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assess performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use three different metrics - RMSE (root mean squared), MAE (mean absolute error), MAPE (mean absolute percentage error) \n",
    "# to evaluate your model performance:\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "print('RMSE:', \n",
    "        ...     # Fill out with your code! \n",
    "      )\n",
    "\n",
    "print('MAE:', \n",
    "        ...     # Fill out with your code! \n",
    "      )\n",
    "\n",
    "print('MAPE:', \n",
    "        ...     # Fill out with your code!                   \n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Snapshots/Updated Values\n",
    "\n",
    "There are also instances where you might want to keep observations seperate between folds, for example when scaling features. \n",
    "\n",
    "In this case, it is recommended that you use `cross_val_fit_predict` with a Pipeline object.\n",
    "\n",
    "However, in case you need to manually implement something where observations must be handled differently across folds, PanelSplit's `gen_snapshots` converts a single df to multiple snapshots of that dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create snapshots of the data using n_splits = (2023 - 2013):\n",
    "\n",
    "panel_split = PanelSplit(      \n",
    "                         \n",
    "                         # Fill out with your code! \n",
    "                         \n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_df = panel_split.gen_snapshots(df, period_col = 'date')\n",
    "print('2 new columns are created: split and snapshot_period:')\n",
    "display(snapshot_df)\n",
    "print(f'before gen_snapshot:{df.shape} after gen_snapshot:{snapshot_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scale our data using `cross_val_fit_transform`. Here each snapshot is scaled separately. We also specify `transform_train=True`, thereby also transforming the training set for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_split = PanelSplit(periods = snapshot_df.date, snapshots = snapshot_df.snapshot_period, n_splits = (2023 - 2013)) # notice the usage of the snapshots argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MinMaxScaler to rescale the data:\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "snapshot_df[features], scalers = panel_split.cross_val_fit_transform(\n",
    "    \n",
    "        # Fill out with your code! \n",
    "    \n",
    "        ) # transform_train transforms the training set for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(snapshot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Instructions:\n",
    "\n",
    "- Go to the **Assessments** tab in iCollege and click on **Assignments.** Submit your solution under the **Homework 3** category.\n",
    "\n",
    "- Report your values for **RMSE, MAE and MAPE** for your model performnace in the Homework submission field in iCollege.\n",
    "\n",
    "- Attach this **executed (!)** Jupyter notebook and submit with your response above in iCollege."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_timeseries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
